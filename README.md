# conda-env-configs

> Infrastructure-as-code for reproducible Conda environments.

This repository defines all development environments used across projects as
versioned `environment.yml` files, with lightweight tooling for rebuilding and
auditing environments.

---

## üéØ Goals

- Reproducible environments
- Explicit version pinning
- Minimal base environment
- Easy rebuilds
- Human-readable dependency intent
- Lightweight automation only
- Visual documentation via Mermaid

---

## üß± Environment Philosophy

Environments are treated like disposable containers:

- Create per project
- Keep small and intentional
- Delete and rebuild freely
- Never pollute `base`

---

## üì¶ Dependency Ownership

Each project maintains its own `requirements.txt`.

This repository defines:

- Python version
- Conda channels
- How project dependencies are installed

All application dependencies must be edited in the project repository, not here.

Environment files reference project requirements directly:

pip:

- -r ../my-project/requirements.txt

---

## üêç Python Baseline

Default Python version:

```txt
python=3.10
```

Individual environments may override when required.

---

## üìÇ Repository Layout

```txt
conda-envs/
‚îú‚îÄ‚îÄ base/ # Minimal tooling only
‚îú‚îÄ‚îÄ core/ # Shared dev stack
‚îú‚îÄ‚îÄ diagrams/ # Mermaid diagrams
‚îú‚îÄ‚îÄ template/ # New env template
‚îú‚îÄ‚îÄ tools/ # Bash automation
|
‚îú‚îÄ‚îÄ example_env/ # your created environments
‚îî‚îÄ‚îÄ test_env/ #
```

---

## ‚öôÔ∏è Common Commands

### Build or update an environment

```bash
./tools/build.sh envname
```

---

### Rebuild an environment cleanly

```bash
./tools/rebuild.sh envname
```

---

## üìå Version Pinning Rules

- Pin Python explicitly.

- Pin critical libraries exactly (ML, CUDA, ABI-sensitive).

- Use ranges for tooling and leaf libraries.

- Avoid committing fully locked transitive graphs to public repositories unless intentional.

---

## üßπ Maintenance

Periodic cleanup:

### 1.

```sh
conda clean -a -y
```

This removes only cached and temporary artifacts.

| Category                    | What gets deleted             | Why it exists                   |
| --------------------------- | ----------------------------- | ------------------------------- |
| üì¶ Package tarballs         | `.tar.bz2` and `.conda` files | Downloaded installers for reuse |
| üß± Extracted package caches | Unpacked package folders      | Speeds up env creation          |
| üß™ Index caches             | Repo metadata                 | Faster package resolution       |
| üóëÔ∏è Temp files               | Partial downloads             | Interrupted installs            |
| üß† Unused caches            | Old cache entries             | Disk bloat                      |

### 2.

```sh
pip cache purge
```

This deletes:

- Downloaded `.whl` files
- Source distributions (`.tar.gz`)
- Cached build wheels
- HTTP cache entries

### 3.

Audit environments:

```sh
conda env list
```

Delete unused environments.

---

# üß± base/environment.yml

> Minimal tooling only ‚Äî no project dependencies.

```yaml
name: base
channels:
  - conda-forge
dependencies:
  - python=3.10
  - conda
  - pip
  - setuptools
  - wheel
```

---

# üìä Environment Dependency Graph

> Auto-generated on every commit.

<!-- DEP_GRAPH_START -->

```mermaid
graph TD

  subgraph cluster_core["core"]
    python_dotenv__1_2_1[python_dotenv==1.2.1]
    pyyaml__6_0_3[pyyaml==6.0.3]
  end

```

<!-- DEP_GRAPH_END -->

---

# üîê Make Scripts Executable

Run once:

```bash
chmod +x tools/*.sh
```

---

## üîÅ Regenerating the Dependency Graph Locally

The dependency graph is generated by scanning all `environment.yml` files and resolving any referenced `requirements.txt` files.

To regenerate locally:

```bash
python tools/generate_graph.py
```

This will:

- Update `diagrams/dependency-graph.mmd`
- Update `diagrams/dependency-clusters.txt`
- Inject the Mermaid graph into this README automatically

Commit the changes after regeneration:

```bash
git add diagrams README.md
git commit -m "Update dependency graph"
git push
```

### ‚ö†Ô∏è Local Filesystem vs GitHub Actions

If an environment references a requirements file that lives **outside this repository**, for example:

```yaml
pip:
  - -r ../../my-project/requirements.txt
```

Then:

- ‚úÖ Local runs will resolve correctly (your filesystem contains the file)
- ‚ùå GitHub Actions will NOT be able to see this file
- ‚ö†Ô∏è The CI graph will fall back to empty or placeholder data

This is expected behavior. GitHub Actions runs in a clean sandbox and only has access to files inside the repository.

#### ‚úîÔ∏è Recommended Usage Patterns

**Template / Public Repositories**

Use placeholder paths or example requirements files inside the repo so CI can generate a demo graph.

**Private / Personal Repositories**

Run `tools/generate_graph.py` locally using your real filesystem paths and commit the generated artifacts manually.
GitHub Actions should not be relied upon for dependency discovery when external paths are involved.

#### üö® CI Overwrite Warning

If GitHub Actions is enabled and commits graph updates automatically:

- Any locally generated graph may be overwritten by CI if external paths cannot be resolved.
- For private repos with real filesystem dependencies, disable CI graph generation or rely exclusively on local generation.

---

## ‚öôÔ∏è Enabling GitHub Auto-Generation (Optional)

This template includes an optional GitHub Action that automatically regenerates the dependency graph on every push to `main`.

To enable:

1. Go to:

```
Repository ‚Üí Settings ‚Üí Variables ‚Üí Actions
```

2. Add:

```
Name: ENABLE_DEP_GRAPH
Value: true
```

To disable, delete the variable or set it to `false`.

If disabled, graphs should be generated manually using:

```bash
python tools/generate_graph.py
```

---

## üß™ Generating Requirements Automatically (Optional)

If you don‚Äôt already maintain a `requirements.txt`, you can bootstrap one using:

```bash
pip install pipreqs
pipreqs .
```

This scans your source tree and infers imported dependencies.

Notes:

- Works well for most projects.
- May miss dynamically imported modules.
- Always review and pin versions intentionally afterward.

This tool is especially useful when onboarding an existing project into this system, but the final dependency list should always be curated intentionally.

---
